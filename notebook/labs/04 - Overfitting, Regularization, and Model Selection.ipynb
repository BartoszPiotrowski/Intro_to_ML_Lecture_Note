{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI-UA 0473 - Introduction to Machine Learning\n",
    "## Monday, February 12, 2018\n",
    "\n",
    "This lab will discuss briefly about *overfitting*, and various approaches to deal with it.\n",
    "\n",
    "### 1. Overfitting\n",
    "\n",
    "To quote [Wikipedia](https://en.wikipedia.org/wiki/Overfitting):\n",
    "> In statistics, **overfitting** is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably\".\n",
    "\n",
    "Several scenarios, and their combinations, would increase the chance of overfitting, hence a larger generalization error:\n",
    "* Lack of data (especially when compared to the size of feature space and/or model capacity)\n",
    "* Inherent noise in data\n",
    "* [Data leakage](https://www.kaggle.com/wiki/Leakage), including using unexpected features correlated to the target variable for prediction (e.g. using object ID to predict object class).\n",
    "\n",
    "Example of overfitting for classification:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png)\n",
    "\n",
    "And another one for regression:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Overfitted_Data.png/300px-Overfitted_Data.png)\n",
    "\n",
    "(Both images taken from Wikipedia)\n",
    "\n",
    "Solutions to aim for a better generalization error includes\n",
    "* Collecting more data.  Often the best solution, **provided that you can afford the cost**.\n",
    "* Cutting off model capacity, the most common form of which is *regularization*, to reduce overfitting.\n",
    "* Validation-set-based model selection, including *early stopping* (to mitigate overfitting) and *hyperparameter search*.\n",
    "\n",
    "The following is a demonstration of logistic regression overfitting on some 2D toy data.  Overfitting is best observed when you use complex models, and/or on high-dimensional features.  But since we have yet come to that, we will see how a simple model would overfit in a low-data regime, even with low-dimensional feature space.\n",
    "\n",
    "First, let's create some toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from autograd import value_and_grad\n",
    "\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "npr.seed(1234)\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "n_dim = 2\n",
    "x_tra, y_tra = make_blobs(n_samples=20, n_features=n_dim, centers=[[1,1],[-1,-1]], shuffle=True)\n",
    "x_val, y_val = make_blobs(n_samples=20, n_features=n_dim, centers=[[1,1],[-1,-1]], shuffle=True)\n",
    "x_tes, y_tes = make_blobs(n_samples=100, n_features=n_dim, centers=[[1,1],[-1,-1]], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mess for visualization...\n",
    "def vis_data(ax, x, y = None, marker='r.', pos_marker='r.', neg_marker='b.'):\n",
    "    if y is None: \n",
    "        y = [None] * len(x)\n",
    "    for x_, y_ in zip(x, y):\n",
    "        if y_ is None:\n",
    "            ax.plot(x_[0], x_[1], marker)\n",
    "        else:\n",
    "            ax.plot(x_[0], x_[1], neg_marker if y_ == 0 else pos_marker)\n",
    "    ax.grid('on')\n",
    "def vis_hyperplane(ax, w, typ='k--', label=None):\n",
    "\n",
    "    lim0 = ax.get_xlim()\n",
    "    lim1 = ax.get_ylim()\n",
    "    m0, m1 = lim0[0], lim0[1]\n",
    "\n",
    "    intercept0 = -(w[0] * m0 + w[-1])/w[1]\n",
    "    intercept1 = -(w[0] * m1 + w[-1])/w[1]\n",
    "    \n",
    "    plt1, = ax.plot([m0, m1], [intercept0, intercept1], typ, label=label)\n",
    "\n",
    "    ax.set_xlim(lim0)\n",
    "    ax.set_ylim(lim1)\n",
    "    \n",
    "    return plt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.subplots()\n",
    "vis_data(ax, x_tra, y_tra, pos_marker='ro', neg_marker='bo')\n",
    "vis_data(ax, x_tes, y_tes, pos_marker='r+', neg_marker='b+')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build a logistic regression model.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Beware**: The following logistic regression implementation is still numerically unstable.  ~~Unfortunately~~ We decided to leave such implementation as one of your homework assignment.  Don't worry - we prepared a guide for deriving a proper implementation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, params):\n",
    "    w, b = params[:-1], params[-1]\n",
    "    return x @ w + b # replace this to np.dot(x, w) + b if @ doesn't work\n",
    "\n",
    "def predict(x, params):\n",
    "    return score(x, params) > 0\n",
    "\n",
    "def logreg_cost(params, x, y, lambda_=0):\n",
    "    s = score(x, params)\n",
    "    prob = 1 / (1 + np.exp(-s)) # sigmoid\n",
    "    cost = -(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n",
    "    return cost.mean() + lambda_ * (params ** 2).sum() # if lambda_ > 0, then we have a regularized model (will come to this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of demonstration and an easier implementation of early stopping, we do not use `scipy.optimize.minimize()` here.  Instead, we perform a normal gradient descent by our own.\n",
    "\n",
    "Recall that given the parameters $\\mathbf{w}$ and the gradient of the cost function $\\nabla_\\mathbf{w} \\mathcal{L}$ w.r.t. the parameters, the update rule for normal gradient descent is\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\gets \\mathbf{w} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is some value picked before the training process (a.k.a. *hyperparameter*).  Usually it is called *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, params, cost_fn, lambda_=0, learning_rate=0.1, n_iter=2000, keep_history=False):\n",
    "    '''\n",
    "    x_train: a 2D array with shape (n_samples, n_features)\n",
    "    y_train: a 1D array of 0's and 1's, with shape (n_samples,)\n",
    "    params: initial parameter array\n",
    "    cost_fn: a cost function we wish to minimize\n",
    "    lambda_: the same lambda_ that will be fed into the cost function.\n",
    "    learning_rate: see above\n",
    "    n_iter: the maximum number of iterations\n",
    "    keep_history: if True, returns a series of historical parameter arrays throughout the optimization instead\n",
    "    \n",
    "    returns a parameter array that minimizes cost_fn(params, x_train, y_train, lambda_) via gradient descent.\n",
    "    '''\n",
    "    # value_and_grad returns a function that compute the cost function and the gradient simultaneously\n",
    "    cost_and_grad = value_and_grad(cost_fn, 0)\n",
    "    best_cost = np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    if keep_history:\n",
    "        history = []\n",
    "    # Iteratively perform gradient descent\n",
    "    for i in range(n_iter):\n",
    "        # compute the cost and gradient\n",
    "        cost, dparams = cost_and_grad(params, x_train, y_train, lambda_=lambda_)\n",
    "        \n",
    "        if best_cost > cost:\n",
    "            best_cost = cost\n",
    "            best_params = params\n",
    "        if keep_history:\n",
    "            history.append(params)\n",
    "        # update\n",
    "        params = params - dparams * learning_rate\n",
    "        \n",
    "    return best_params if not keep_history else np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def train_and_animate(x_tra, y_tra, x_tes, y_tes, lambda_=0):\n",
    "    n_dim = x_tra.shape[1]\n",
    "    params0 = npr.randn(n_dim + 1) * 0.01\n",
    "    params_history = train_model(x_tra, y_tra, params0, logreg_cost, keep_history=True, lambda_=lambda_)\n",
    "\n",
    "    # Yet another mess for visualization all the way to the end...\n",
    "    fig, ax = plot.subplots()\n",
    "    vis_data(ax, x_tra, y_tra, pos_marker='ro', neg_marker='bo')\n",
    "    vis_data(ax, x_tes, y_tes, pos_marker='r+', neg_marker='b+')\n",
    "    line = vis_hyperplane(ax, params_history[0], label='decision boundary')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    def set_intercepts(line, ax, w):\n",
    "        lim0 = ax.get_xlim()\n",
    "        lim1 = ax.get_ylim()\n",
    "        m0, m1 = lim0[0], lim0[1]\n",
    "\n",
    "        intercept0 = -(w[0] * m0 + w[-1])/w[1]\n",
    "        intercept1 = -(w[0] * m1 + w[-1])/w[1]\n",
    "        line.set_data([m0, m1], [intercept0, intercept1])\n",
    "\n",
    "    def animate(t):\n",
    "        set_intercepts(line, ax, params_history[t])\n",
    "        return line,\n",
    "\n",
    "    def init():\n",
    "        set_intercepts(line, ax, params_history[0])\n",
    "        return line,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, np.arange(0, 2000, 10), init_func=init,\n",
    "                                  interval=100, blit=False)\n",
    "    return ani\n",
    "    \n",
    "ani = train_and_animate(x_tra, y_tra, x_tes, y_tes)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regularization\n",
    "\n",
    "Regularization is a way to limit the model capacity by either constraining or penalizing the weight magnitudes.  It can be applied to any parametric models.\n",
    "\n",
    "The most common regularization is probably *L2 regularization*, which adds a penalty proportional to the L2-norm of the weight:\n",
    "$$\n",
    "J_\\mathrm{reg}(D_\\mathrm{tra}, \\mathbf{w}) = J(D_\\mathrm{tra}, \\mathbf{w}) + \\dfrac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert^2\n",
    "$$\n",
    "It is also called *weight decay* presumably because the gradient of $\\mathbf{w}$ has a correction term proportional to $\\mathbf{w}$ itself:\n",
    "$$\n",
    "\\nabla_\\mathbf{w} J_\\mathrm{reg}(D_\\mathrm{tra}, \\mathbf{w}) = \\nabla_\\mathbf{w} J(D_\\mathrm{tra}, \\mathbf{w}) + \\lambda \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Intuitively, the term $\\dfrac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert^2$ encodes our *belief* that the parameters should not be too large.  The belief is stronger if $\\lambda$ is larger.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Math**: If we consider the original cost function $J$ as some negative log-likelihood function (such as the one being used in logistic regression), and regard $\\dfrac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert^2$ as yet another negative log-likelihood term, we can see that L2 regularization is equivalent to saying that \"*We believe our parameters to come from a Gaussian centered at 0*\".\n",
    "</div>\n",
    "\n",
    "Now let's run another experiment on the same training set and test set, but with $\\lambda$ cranked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = train_and_animate(x_tra, y_tra, x_tes, y_tes, 0.1)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that, indeed the regularized logistic regression does generalize better.  However, L2 regularization is not something applicable to any situation.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "**Question**: What if we shift the entire dataset away from the origin (say, 2 units up and 2 units right)?  Run the following cell and see what happens.  What should we do before training so that regularization can have the chance to help?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = train_and_animate(x_tra + 2, y_tra, x_tes + 2, y_tes, 0.1)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.subplots()\n",
    "vis_data(ax, x_tra, y_tra, pos_marker='ro', neg_marker='bo')\n",
    "vis_data(ax, x_val, y_val, pos_marker='r^', neg_marker='b^')\n",
    "vis_data(ax, x_tes, y_tes, pos_marker='r+', neg_marker='b+')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Validation-set-based model selection\n",
    "\n",
    "Usually, what we care about in machine learning is the performance on the unseen test set, rather than the training set where the model is tuned.  If we split out some of the training set to form a *validation set*, and train the model on the rest of the training set, we can then select a model which has the least *validation error*, and hope that the test error is small as well.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Beware**: As soon as you *pick* the model with best validation error, the error itself is no longer a good estimate of (expected) test error.\n",
    "</div>\n",
    "\n",
    "Both *early stopping* and *hyperparameter search* are based on this intuition.\n",
    "\n",
    "#### 3.1 Early Stopping\n",
    "\n",
    "The idea of early stopping is simple: if we didn't see any significant improvement on validation error for a certain period, we say \"this is it\" and stop.  However, different people have different definitions on which improvements are *significant*, and how long a *certain* period should be.  The fluctuation of validation error further complicates the implementation of early stopping.\n",
    "\n",
    "As a demonstration, we stop after the model did not improve over the best validation *accuracy* in 30 gradient steps.  Though in the following implementation, we go all the way to the end anyway, to better visualize the difference between an early-stopping-selected model, the model with \"truly\"-best validation error, and the model with best training loss.\n",
    "\n",
    "If you are interested in which early stopping criteria is more commonly used, see [here](https://keras.io/callbacks/#earlystopping) and [here](http://deeplearning.net/tutorial/gettingstarted.html#early-stopping).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Caveat**: In this lab, the validation set is separately generated rather than picking from the training set.  But often times, you are not able to do so (e.g. with a fixed-size training set and no easy way to acquire new samples).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Beware**: Make sure that the validation set does not appear in the dataset for optimization.  Make sure that the test set does not appear in the dataset for optimization **AND** validation.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Please answer the question in the cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(\n",
    "    x_train, y_train, x_valid, y_valid, params, cost_fn, valid_fn, lambda_=0, learning_rate=0.1, n_iter=2000, keep_history=False):\n",
    "    '''\n",
    "    x_train: a 2D array with shape (n_samples, n_features)\n",
    "    y_train: a 1D array of 0's and 1's, with shape (n_samples,)\n",
    "    x_valid, y_valid: same, but for validation\n",
    "    params: initial parameter array\n",
    "    cost_fn: a cost function we wish to minimize\n",
    "    valid_fn: a cost function for validation error\n",
    "    lambda_: the same lambda_ that will be fed into the cost function.\n",
    "    learning_rate: see above\n",
    "    n_iter: the maximum number of iterations\n",
    "    keep_history: if True, returns a series of historical parameter arrays throughout the optimization instead\n",
    "    \n",
    "    returns a parameter array that minimizes cost_fn(params, x_train, y_train, lambda_) via gradient descent.\n",
    "    '''\n",
    "    # value_and_grad returns a function that compute the cost function and the gradient simultaneously\n",
    "    cost_and_grad = value_and_grad(cost_fn, 0)\n",
    "    best_validation_cost = np.inf\n",
    "    early_stopped = False\n",
    "    best_params = params\n",
    "    early_stopped_params = params\n",
    "    best_iter = 0\n",
    "    \n",
    "    if keep_history:\n",
    "        history = []\n",
    "        early_stopping_history = []\n",
    "        best_validation_history = []\n",
    "    # Iteratively perform gradient descent\n",
    "    for i in range(n_iter):\n",
    "        # compute the cost and gradient\n",
    "        cost, dparams = cost_and_grad(params, x_train, y_train, lambda_=lambda_)\n",
    "        # QUESTION:\n",
    "        # Why are we using a separate validation error function, different from the training cost function here?\n",
    "        validation_cost = valid_fn(params, x_valid, y_valid)\n",
    "        \n",
    "        if best_validation_cost > validation_cost:\n",
    "            best_validation_cost = validation_cost\n",
    "            best_iter = i\n",
    "            best_params = params\n",
    "            if not early_stopped:\n",
    "                early_stopped_params = params\n",
    "        elif i > best_iter + 30:\n",
    "            early_stopped = True\n",
    "            # Usually we should break out but I'm continuing the training for visualization purposes\n",
    "            #break\n",
    "\n",
    "        if keep_history:\n",
    "            history.append(params)\n",
    "            early_stopping_history.append(early_stopped_params)\n",
    "            best_validation_history.append(best_params)\n",
    "        # update\n",
    "        params = params - dparams * learning_rate\n",
    "        \n",
    "    return early_stopped_params if not keep_history else (history, early_stopping_history, best_validation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy_error(params, x, y):\n",
    "    # Not very intuitive, but this essentially means the higher the accuracy the better,\n",
    "    # since the training procedure above picks the model with the *least* validation error.\n",
    "    return 1 - accuracy_score(y, predict(x, params))\n",
    "\n",
    "def train_with_early_stopping_and_animate(x_tra, y_tra, x_val, y_val, x_tes, y_tes, lambda_=0):\n",
    "    n_dim = x_tra.shape[1]\n",
    "    params0 = npr.randn(n_dim + 1) * 0.01\n",
    "    params_history, early_stopping_history, best_validation_history = train_model_with_early_stopping(\n",
    "        x_tra, y_tra, x_val, y_val, params0, logreg_cost, accuracy_error, keep_history=True, lambda_=lambda_)\n",
    "\n",
    "    # Again a gigantic piece for visualization...\n",
    "    fig, ax = plot.subplots()\n",
    "    vis_data(ax, x_tra, y_tra, pos_marker='ro', neg_marker='bo')\n",
    "    vis_data(ax, x_val, y_val, pos_marker='r^', neg_marker='b^')\n",
    "    vis_data(ax, x_tes, y_tes, pos_marker='r+', neg_marker='b+')\n",
    "    line = vis_hyperplane(ax, params_history[0], label='decision boundary')\n",
    "    early_stopping_line = vis_hyperplane(ax, early_stopping_history[0], 'm-', label='early stopping')\n",
    "    best_validation_line = vis_hyperplane(ax, best_validation_history[0], 'g-.', label='best validation')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    def set_intercepts(line, ax, w):\n",
    "        lim0 = ax.get_xlim()\n",
    "        lim1 = ax.get_ylim()\n",
    "        m0, m1 = lim0[0], lim0[1]\n",
    "\n",
    "        intercept0 = -(w[0] * m0 + w[-1])/w[1]\n",
    "        intercept1 = -(w[0] * m1 + w[-1])/w[1]\n",
    "        line.set_data([m0, m1], [intercept0, intercept1])\n",
    "\n",
    "    def animate(t):\n",
    "        set_intercepts(line, ax, params_history[t])\n",
    "        set_intercepts(early_stopping_line, ax, early_stopping_history[t])\n",
    "        set_intercepts(best_validation_line, ax, best_validation_history[t])\n",
    "        return line, early_stopping_line, best_validation_line\n",
    "\n",
    "    def init():\n",
    "        set_intercepts(line, ax, params_history[0])\n",
    "        set_intercepts(early_stopping_line, ax, early_stopping_history[0])\n",
    "        set_intercepts(best_validation_line, ax, best_validation_history[0])\n",
    "        return line, early_stopping_line, best_validation_line\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, np.arange(0, 2000, 10), init_func=init,\n",
    "                                  interval=100, blit=False)\n",
    "    return ani\n",
    "    \n",
    "ani = train_with_early_stopping_and_animate(x_tra, y_tra, x_val, y_val, x_tes, y_tes)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Hyperparameter search\n",
    "\n",
    "Hyperparameter search is yet another simple, brute-force-ish idea to opt for a better generalization error.  Basically, we specify the range to search for each hyperparameter, no matter if they are continuous or discrete, and we either exhaustively try every combination (*grid search*), or test a bunch of those (*random search*), and choose the configuration with the best validation error.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Please modify the following function, so that it exhaustively try every $\\lambda$ given in the list, returning the best model.  You don't need to save history for anything.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping_and_grid_search(\n",
    "    x_train, y_train, x_valid, y_valid, params, cost_fn, valid_fn, lambda_=0, learning_rate=0.1, n_iter=2000):\n",
    "    '''\n",
    "    x_train: a 2D array with shape (n_samples, n_features)\n",
    "    y_train: a 1D array of 0's and 1's, with shape (n_samples,)\n",
    "    x_valid, y_valid: same, but for validation\n",
    "    params: initial parameter array\n",
    "    cost_fn: a cost function we wish to minimize\n",
    "    valid_fn: a cost function for validation error\n",
    "    lambda_: a list of lambda's\n",
    "    learning_rate: see above\n",
    "    n_iter: the maximum number of iterations\n",
    "    \n",
    "    returns a parameter array that minimizes cost_fn(params, x_train, y_train, lambda_) via gradient descent.\n",
    "    '''\n",
    "    # value_and_grad returns a function that compute the cost function and the gradient simultaneously\n",
    "    cost_and_grad = value_and_grad(cost_fn, 0)\n",
    "    best_validation_cost = np.inf\n",
    "    early_stopped = False\n",
    "    best_params = params\n",
    "    early_stopped_params = params\n",
    "    best_iter = 0\n",
    "    \n",
    "    # Iteratively perform gradient descent\n",
    "    for i in range(n_iter):\n",
    "        # compute the cost and gradient\n",
    "        cost, dparams = cost_and_grad(params, x_train, y_train, lambda_=lambda_)\n",
    "        assert cost.ndim == 0\n",
    "        validation_cost = valid_fn(params, x_valid, y_valid)\n",
    "        \n",
    "        if best_validation_cost > validation_cost:\n",
    "            best_validation_cost = validation_cost\n",
    "            best_iter = i\n",
    "            best_params = params\n",
    "            if not early_stopped:\n",
    "                early_stopped_params = params\n",
    "        elif i > best_iter + 30:\n",
    "            early_stopped = True\n",
    "            # Usually we should break out but I'm continuing the training for visualization purposes\n",
    "            #break\n",
    "\n",
    "        # update\n",
    "        params = params - dparams * learning_rate\n",
    "        \n",
    "    return early_stopped_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params0 = npr.randn(n_dim + 1) * 0.01\n",
    "params = train_model_with_early_stopping_and_grid_search(\n",
    "    x_tra, y_tra, x_val, y_val, params0, logreg_cost, accuracy_error, lambda_=[0, 0.1])\n",
    "\n",
    "fig, ax = plot.subplots()\n",
    "vis_data(ax, x_tra, y_tra, pos_marker='ro', neg_marker='bo')\n",
    "vis_data(ax, x_val, y_val, pos_marker='rx', neg_marker='b^')\n",
    "vis_data(ax, x_tes, y_tes, pos_marker='r+', neg_marker='b+')\n",
    "line = vis_hyperplane(ax, params, label='decision boundary')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. See also\n",
    "\n",
    "`scikit-learn` has a whole set of tools for model selection, with hyperparameter search and (cross-)validation wrapped up as `scikit-learn` models; see http://scikit-learn.org/stable/modules/grid_search.html for details.  Notably, they have `sklearn.model_selection.GridSearchCV` and `sklearn.model_selection.RandomizedSearchCV` to do the dirty work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
